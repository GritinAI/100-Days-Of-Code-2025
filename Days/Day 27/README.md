
# Day 0ï¸âƒ£2ï¸âƒ£7ï¸âƒ£: Pre-LLMs II (Embeddings ğŸ”¢ğŸ”¢ğŸ”¢ II)

---

$$\text{"... And you shall know a word by the company it keeps."}$$
$$\text{John B. Firth}$$

---

## Synopsis

- [Day 0ï¸âƒ£2ï¸âƒ£7ï¸âƒ£: Pre-LLMs II (Embeddings ğŸ”¢ğŸ”¢ğŸ”¢ II)](#day-0ï¸âƒ£2ï¸âƒ£7ï¸âƒ£-pre-llms-ii-embeddings--ii)
  - [Synopsis](#synopsis)
  - [Embeddings ğŸ”¢ğŸ”¢ğŸ”¢](#embeddings-)
  - [Word Representation Schemes](#word-representation-schemes)
    - [Phase 1ï¸âƒ£: Integers ğŸ”¢](#phase-1ï¸âƒ£-integers-)
      - [Pros ğŸ‘](#pros-)
      - [Cons ğŸ‘](#cons-)
    - [Phase 2ï¸âƒ£: Vectors 1.0 ğŸ”¢ğŸ”¢ğŸ”¢ (One-hot Encoding \[0ï¸âƒ£0ï¸âƒ£0ï¸âƒ£1ï¸âƒ£0ï¸âƒ£\])](#phase-2ï¸âƒ£-vectors-10--one-hot-encoding-0ï¸âƒ£0ï¸âƒ£0ï¸âƒ£1ï¸âƒ£0ï¸âƒ£)
      - [Pros ğŸ‘](#pros--1)
      - [Cons ğŸ‘](#cons--1)
    - [Phase 3ï¸âƒ£: Vectors 2.0 ğŸ”¢ğŸ”¢ğŸ”¢ (Dense Vectors \[ğŸ”¢ğŸ”¢ğŸ”¢ğŸ”¢ğŸ”¢\])](#phase-3ï¸âƒ£-vectors-20--dense-vectors-)
      - [Pros ğŸ‘](#pros--2)
      - [Cons ğŸ‘](#cons--2)
    - [Phase 4ï¸âƒ£: Vectors 3.0 ğŸ”¢ğŸ”¢ğŸ”¢ (Embeddings \[ğŸ”¢ğŸ”¢ğŸ”¢ğŸ”¢ğŸ”¢\])](#phase-4ï¸âƒ£-vectors-30--embeddings-)
      - [Pros ğŸ‘](#pros--3)
      - [Cons ğŸ‘](#cons--3)
  - [Generating Embeddings](#generating-embeddings)
    - [1ï¸âƒ£ Feed-forward networks (FFNs)](#1ï¸âƒ£-feed-forward-networks-ffns)
      - [ğŸ…°ï¸ Data](#ï¸-data)
      - [ğŸ…±ï¸ Parameters ğŸ”¢](#ï¸-parameters-)
      - [ğŸ”­ Observations ğŸ…°ï¸ + ğŸ…±ï¸ ğŸ”­](#-observations-ï¸--ï¸-)
    - [2ï¸âƒ£ Embedding Tables](#2ï¸âƒ£-embedding-tables)
  - [Summary](#summary)
  - [Further Materials](#further-materials)
  - [Spotlight of the Day](#spotlight-of-the-day)

---

## Embeddings ğŸ”¢ğŸ”¢ğŸ”¢

From the last lesson, we learnt that embeddings are essential to providing our LLMs with understanding because:

1ï¸âƒ£ They represent the meaning of our tokens ğŸ”µğŸ”´ in vector format and

2ï¸âƒ£ They represent this meaning in multiple dimensions (or perspectives), making for richer information.

What we did not discuss, however, was how these embeddings ğŸ”¢ğŸ”¢ğŸ”¢ come to be. How do we derive them? Or do they fall from heaven ğŸª½ğŸ˜€?

---

## Word Representation Schemes

Even before the advent of embeddings ğŸ”¢ğŸ”¢ğŸ”¢, we have always needed to represent words using numbers ğŸ”¢, since those are all a computer understands. But how have we gone about this historically?

### Phase 1ï¸âƒ£: Integers ğŸ”¢

Prior to embeddings as we know them today, we have always represented words with numbers. As discussed in the last lesson, we would start by representing every word with a unique integer ğŸ”¢ index.

#### Pros ğŸ‘

1ï¸âƒ£ Easy and fast to implement.

#### Cons ğŸ‘

1ï¸âƒ£ Best for identifying words, no meaning attached to simple integer number ğŸ”¢.

2ï¸âƒ£ Words in any language are related to one another i.e., we do not use words in isolation. Rather, we use them in conjunction with other words that either emphasize on or provide contrast to their meaning. Representing words with simple integers ğŸ”¢ cannot achieve this.

---

### Phase 2ï¸âƒ£: Vectors 1.0 ğŸ”¢ğŸ”¢ğŸ”¢ (One-hot Encoding [0ï¸âƒ£0ï¸âƒ£0ï¸âƒ£1ï¸âƒ£0ï¸âƒ£])

The next step was to evolve the word representation method from integers ğŸ”¢ $\Rightarrow$ vectors ğŸ”¢ğŸ”¢ğŸ”¢. This would allow us to solve the issue with integer ğŸ”¢ representation where words exist in isolation.

The first approach to come up is known as **One-hot Encoding**. The steps are outlined below:

1ï¸âƒ£ Create the vocabulary.

2ï¸âƒ£ Get the vocabulary size as $N$ e.g., $N = 5$.

3ï¸âƒ£ For each word, $W$...

3ï¸âƒ£ğŸ…°ï¸ Create a vector ($V_W$) of size $N$ filled with zeros e.g., $V_W = \{ 0, 0, 0, 0, 0 \}$

3ï¸âƒ£ğŸ…±ï¸ Get the integer ğŸ”¢ index, $I_W$, of word $W$ and fill vector $V_W$ with a 1 at that index $I_W$. For example:

---
$\text{Given ...}$
$$N = 5$$
$$I_W = 2$$
$$V_W = \{ 0, 0, 0, 0, 0 \}$$

$\text{Then ...}$
$$V_W[I_W] = 1$$
$$V_W = \{ 0, 0, 1, 0, 0 \}$$

---

#### Pros ğŸ‘

1ï¸âƒ£ Easy and fast to implement.

2ï¸âƒ£ Leverages vectors, so more information.

#### Cons ğŸ‘

1ï¸âƒ£ One-hot vectors are mostly zeros, so limited information.

2ï¸âƒ£ Words in any language are related to one another i.e., we do not use words in isolation. Rather, we use them in conjunction with other words that either emphasize on or provide contrast to their meaning. Representing words with simple one-hot vectors only represents the position of the word, and cannot achieve this.

---

### Phase 3ï¸âƒ£: Vectors 2.0 ğŸ”¢ğŸ”¢ğŸ”¢ (Dense Vectors [ğŸ”¢ğŸ”¢ğŸ”¢ğŸ”¢ğŸ”¢])

As stated above, one-hot vector encoding was limited by irs sparse nature. How do we moe past it?

---

$$ \text{Answer: Distributed Representations ğŸ˜„!} $$

---

The idea of distributed representations is that words can have their individual meaning, but that they can also derive some of their meaning from other words they appear with!

This idea is so powerful that it gave rise to a new approach called **Dense Vectors**. The idea behind this was to derive vectors for words, not only based on the words themselves, but also based on the words they are likely to appear with. Examples of projects that applied this method with success include:

1ï¸âƒ£ Word2Vec

2ï¸âƒ£ GloVe

3ï¸âƒ£ NLTK

---

#### Pros ğŸ‘

1ï¸âƒ£ Easy and fast to implement.

2ï¸âƒ£ Leverages vectors, so more information.

#### Cons ğŸ‘

1ï¸âƒ£ Requires time and data to train.

---

### Phase 4ï¸âƒ£: Vectors 3.0 ğŸ”¢ğŸ”¢ğŸ”¢ (Embeddings [ğŸ”¢ğŸ”¢ğŸ”¢ğŸ”¢ğŸ”¢])

This is the present state-of-the-art for word representation. This is exactly what we set out to learn about.

---

#### Pros ğŸ‘

1ï¸âƒ£ Easy and fast to implement.

2ï¸âƒ£ Leverages vectors, so more information.

3ï¸âƒ£ We can control the number of embedding dimensions.

#### Cons ğŸ‘

1ï¸âƒ£ Requires time and data to train.

---

## Generating Embeddings

So, now we know how we got here, to a place where embeddings have become integral to machine learning. Now its time to answer the million-dollar question:

---

$$ \text{Question: How do we generate embeddings? Where do they come from?} $$

---

The answer is actually simple:

---

$$ \text{Answer: From an embedding lookup table $E$ that we train ğŸ˜„!} $$

---

Don't worry ğŸ˜Œ! We'll understand this! But first, let us learn about feed-forward networks.

### 1ï¸âƒ£ Feed-forward networks (FFNs)

The feed-forward network (FFN), also known as multilayer perceptron (MLP), is one of the simplest neural networks ğŸ§  that can be made. It simply processes information by multiplying at least two matrices together, one of which represents the data, and the other represents the parameters ğŸ”¢! Let us look at a simple example:

#### ğŸ…°ï¸ Data

Assume we have the data for 5 customers, each described by their age, salary and years of experience. Recall from our discussion on vectors that this gives us three dimensions across which we can describe our employees: 1ï¸âƒ£ age, 2ï¸âƒ£ salary, and 3ï¸âƒ£ years of experience. We can therefore represent each employee as a vector.

For instance, assume we have an employee named **Victor**, who is aged 39 years, and who has worked with us for 5 years. Representing Victor's information in vector form (Victor, vector, get it ğŸ˜œ!) would be like this:

---

$$\text{$x_i$ = [Victor, 39, 5]}$$

---

With more employees, we end up with multiple vectors:

---

$$ \text{$x_1$ =  [Victor, 39, 5]} $$
$$ \text{$x_2$ =  [Joseph, 26, 3]}$$
$$ \text{$x_3$ =  [Hannah, 29, 7]}$$
$$ \text{$x_4$ =  [Sharon, 31, 3]}$$
$$ \text{$x_5$ =  [Halima, 37, 7]}$$

---

Just as we can combine simple numbers to give vectors, we can also combine vectors to give matrices. This means we can represent all this data on our employees as a matrix $X$ of shape $5, 3$.

---

$$ X \in \R^{(5,3)}$$

---

#### ğŸ…±ï¸ Parameters ğŸ”¢

Next, we need the parameters ğŸ”¢ $W$ we wish to multiply our data with. From matrix multiplication, we know that we can only multiply matrices $X$ and $W$ if:

---

$$ X \in \R^{(5,3)}$$
$$ W \in \R^{(3,N)}$$

where $N$ is an arbitrary number we can choose.

---

Let $N = 10$. Then:

---

$$ X \in \R^{(5,3)}$$
$$ W \in \R^{(3,10)}$$

---

**Note**: In this example, if we choose $N$ to be 10, then $W$ will contain a total of $3 \times 10 = 30$ parameters ğŸ”¢.

Matrix multiplication between $X$ and $W$ gives:

$$ X \times W = X^{'} \in \R^{(5,10)}$$

To complete the FFN, all we need is to add a bias term, $b$:

$$ Y = (X \times W + b) = (X^{'} + b) \in \R^{(5,10)}$$

And with this, we have the equation for a feed-forward neural network ğŸ§ !

---

**Note 1ï¸âƒ£**: Sometimes, the bias term $b$ is simply 0 i.e., $b = 0$.

**Note 2ï¸âƒ£**: The bias term $b$ also counts as a parameter ğŸ”¢.

---

#### ğŸ”­ Observations ğŸ…°ï¸ + ğŸ…±ï¸ ğŸ”­

Now, did we observe something from our recent exploration? I'll give a hint again:

$$ Y = (X \times W + b) = (X^{'} + b) \in \R^{(5,10)}$$

---

$$ \text{Answer: The FFN equation looks like the equation of a straight line ğŸ˜®!} $$

---

In fact, it doesn't just look like the straight line equation; it **IS** the straight line equation, only instead of simple numbers, it uses vectors ğŸ˜€!

Now that we know this ...

$$ \text{Question ğŸ¤” \#1: What was the point of learning about FFNs?} $$
$$ \text{Question ğŸ¤” \#2: What was that thing about embedding lookup tables again ğŸ«¤?} $$

### 2ï¸âƒ£ Embedding Tables

Recall the equation for an FFN?

---

$$ Y = XW + b$$

---

Let us simplify it and make three 3ï¸âƒ£ğŸ‘Œchanges. First, let us make the bias term $b$ zero i.e., $b = 0$. In that case:

---

$$ Y = XW + b = XW + 0 = XW$$

---

Secondly, get the vocabulary size as $N_v$. In the case of our example above, let $N_v = 5$, since we have 5 employees. Then let us set the size of or parameter ğŸ”¢ matrix as:

---

$$ W \in \R^{(N_v,10)}$$
$$ W \in \R^{(5,10)}$$

---

Great! Next, let us make $X$ a one-hot matrix, i.e., a collection of one-hot vectors ğŸ˜€ where the vector size is $N_v$! And let us make one final simplification: let us rename $W$ as $E$.

Once we have this, we can represent our embedding layer:

---

$$ Y = XE$$

---

This is the embedding layer, $L_E$:

---

$$ Y = L_E(X) = XE$$

---

To see the practical implementation, please checkout the notebook attached: [embedding notebook ğŸ“’](embeddings_tutorial.ipynb).

---

## Summary

So, we now know how embeddings can be useful. And we know how they are generated.

Now we can see about understanding the structure of LLMs. How did they come to be? How are they built? What are their components ğŸŸ¦ğŸ”µğŸŸ©ğŸŸª (apart from the parameters ğŸ”¢)?

---

## Further Materials

1. [Vector embeddings tutorial (YouTube) ğŸ“½ï¸](https://www.youtube.com/watch?v=yfHHvmaMkcA)
2. [A Beginner's Guide to Vector Embeddings (YouTube) ğŸ“½ï¸](https://www.youtube.com/watch?v=NEreO2zlXDk)
3. [Tokens & Embeddings (Blog) ğŸ“](https://rohitkhattar.substack.com/p/tokens-and-embeddings-the-hidden)
4. [From Tokens to Embeddings (Blog) ğŸ“](https://ml-digest.com/architecture-training-of-the-embedding-layer-of-llms/)
5. [Evolution of Embeddings (Blog) ğŸ“](https://www.francescatabor.com/articles/2025/3/8/llm-bootcamp-module-3-evolution-of-embeddings)
6. [The Evolution of Embeddings (Blog) ğŸ“](https://blog.dailydoseofds.com/p/the-evolution-of-embeddings)

---

## Spotlight of the Day

<br>

<center><img src=27.png alt="Spotlight Image"></img></center>
