
# Day 0ï¸âƒ£2ï¸âƒ£6ï¸âƒ£: Pre-LLMs II (Embeddings ğŸ”¢ğŸ”¢ğŸ”¢ I)

---

$$\text{"Words have meaning."}$$
$$\text{Antonin Scalia}$$

---

## Synopsis

- [Day 0ï¸âƒ£2ï¸âƒ£6ï¸âƒ£: Pre-LLMs II (Embeddings ğŸ”¢ğŸ”¢ğŸ”¢ I)](#day-0ï¸âƒ£2ï¸âƒ£6ï¸âƒ£-pre-llms-ii-embeddings--i)
  - [Synopsis](#synopsis)
  - [From Tokens ğŸ”µğŸ”´ to Embeddings ğŸ”¢ğŸ”¢ğŸ”¢](#from-tokens--to-embeddings-)
  - [Phase 1ï¸âƒ£: Vectors ğŸ”¢ğŸ”¢ğŸ”¢](#phase-1ï¸âƒ£-vectors-)
  - [Phase 2ï¸âƒ£: Vector ğŸ”¢ğŸ”¢ğŸ”¢ Similarity âš–ï¸](#phase-2ï¸âƒ£-vector--similarity-ï¸)
    - [Vector ğŸ”¢ğŸ”¢ğŸ”¢ Comparison âš–ï¸ 1.0](#vector--comparison-ï¸-10)
      - [Assumptions](#assumptions)
    - [Vector ğŸ”¢ğŸ”¢ğŸ”¢ Comparison âš–ï¸ 2.0](#vector--comparison-ï¸-20)
  - [Phase 3ï¸âƒ£: Embeddings ğŸ”¢ğŸ”¢ğŸ”¢ as Vectors ğŸ”¢ğŸ”¢ğŸ”¢](#phase-3ï¸âƒ£-embeddings--as-vectors-)
  - [Summary](#summary)
  - [Further Materials](#further-materials)
  - [Spotlight of the Day](#spotlight-of-the-day)

---

## From Tokens ğŸ”µğŸ”´ to Embeddings ğŸ”¢ğŸ”¢ğŸ”¢

In the last lesson, we saw how we could help our LLM uniquely identify each token ğŸ”µğŸ”´ by assigning a unique integer value to each. Since LLMs (and computers ğŸ’»ğŸ–¥ï¸) only speak numbers, this also makes it easier to process our text ğŸ”¡. There is a challenge though: the LLM can identify the tokens ğŸ”µğŸ”´ and represent them as numbers ğŸ”¢ now. But it has no idea of the meaning of these tokens ğŸ”µğŸ”´.

We need to fix this problem, and embeddings ğŸ”¢ğŸ”¢ğŸ”¢ will help us do that.

---

## Phase 1ï¸âƒ£: Vectors ğŸ”¢ğŸ”¢ğŸ”¢

Information about the world ğŸŒ around us can be represented as numbers ğŸ”¢ in one way or the other. For instance, age (80 years ğŸ‘µ), national GDPğŸ¤‘, income ğŸ’· (e.g., $10,000), account numbers ğŸ’³, health details ğŸ©º, drug prescriptions ğŸ’Š (e.g., 5 capsules) and so much more can be represented as simple numbers ğŸ”¢. However, simple numbers are not always enough. Let us visit an example.

Imagine we have two materials $A$ and $B$, and that they weigh $\text{3 kg}$ and $\text{15 kg}$ respectively. Let us ask a question:

---

$$\text{Question: Which material is LARGER? Material A or B?}$$

---

The obvious answer here would be $B$! But what if I add in another piece of information ğŸ˜? What if I was to tell you that in litres, material $A$ has a volume of $\text{30 â„“}$ (e.g., a plastic bucket) while $B$ has a volume of $\text{4 â„“}$ (e.g., a metal hammer)? Would the answer still remain the same ğŸ˜€?

Of course not! The mass in $\text{kg}$ tells us the amount of stuff each material contains and how much it weights, but it does not tell us the amount of space it takes up; the volume does that ğŸ˜€! This is the problem with simple numbers:

---

$$ \text{ğŸ’¡ INSIGHT ğŸ’¡: Simple numbers ğŸ”¢ on their own do not tell the whole story! They only give a perspective.} $$

---

Just as we saw in the example above, we needed both the mass and volume to have a better understanding of the materials involved. Each of these (i.e., mass and volume) are simple numbers on their own. But when combined, they improve our understanding of the materials involved from different perspectives. Each of these perspectives is known as a ***Dimension***, and we can keep adding more simple numbers to explain the materials in more perspectives (i.e., dimensions) e.g., melting point, boiling point, density, etcetera.

When we do this, our materials go from being represented with simple numbers ğŸ”¢ like this $\textbf{(I)}$ ...

---

$$ \textbf{(I)} $$
$$ \text{A = 3 kg} $$
$$ \text{B = 15 kg} $$

---

... to this $\textbf{(II)}$:

---

$$ \textbf{(II)} $$
$$ \text{A = [3 kg, 30 â„“]} \rightarrow \text{A = [3, 30]}$$
$$ \text{B = [15 kg, 4 â„“]} \rightarrow \text{B = [15, 4]}$$

---

As can be seen, each material is now represented by a list of numbers [ğŸ”¢ğŸ”¢ğŸ”¢]. Each number in the list represents a dimension along which we are trying to describe the material.

The first approach i.e.,  $\textbf{(I)}$ uses simple numbers known as ***scalars*** or ***scalar quantities***, while the second approach i.e., $\textbf{(II)}$ uses numbers we call ***vectors***, or ***vector quantities***.

---

## Phase 2ï¸âƒ£: Vector ğŸ”¢ğŸ”¢ğŸ”¢ Similarity âš–ï¸

### Vector ğŸ”¢ğŸ”¢ğŸ”¢ Comparison âš–ï¸ 1.0

As we have agreed above, vectors ğŸ”¢ğŸ”¢ğŸ”¢ allow us to represent information in a more complete manner compared to simple numbers. But there is something else. With simple numbers, we could easily conclude at a glance that two objects were the same. Is that the same with vectors ğŸ”¢ğŸ”¢ğŸ”¢? If not how do we know two objects are the same? Let us explore this idea a bit.

Let us have three extra materials $C$, $D$ and $E$, weighing $\text{3 kg}$, $\text{7.5 kg}$, $\text{7.5 kg}$ respectively. Let us tabulate the mass and volumes of our examples:

| Material | Mass | Volume |
|----------|------|--------|
| A | $\text{3 kg}$ | $\text{30 â„“}$ |
| B | $\text{15 kg}$ | $\text{4 â„“}$ |
| C | $\text{3 kg}$ | $\text{27 â„“}$ |
| D | $\text{7.5 kg}$ | $\text{75 â„“}$ |
| E | $\text{7.5 kg}$ | $\text{2 â„“}$ |

---

$$ \textbf{(III)} $$
$$ \text{A = [3 kg, 30 â„“]} \rightarrow \text{A = [3, 30]}$$
$$ \text{B = [15 kg, 4 â„“]} \rightarrow \text{B = [15, 4]}$$
$$ \text{C = [3 kg,   27 â„“]} \rightarrow \text{C = [3,   27]}$$
$$ \text{D = [7.5 kg, 75 â„“]} \rightarrow \text{D = [7.5, 75]}$$
$$ \text{E = [7.5 kg,  2 â„“]} \rightarrow \text{E = [7.5, 2]}$$

If we compare $A$ and $B$ to $C$, $D$ and $E$ on the basis of mass, we can say the following:

---

$$ \text{C = A} $$
$$ \text{D = 2.5 A} $$
$$ \text{E = 2.5 A} $$
$$ \text{C = 0.2 B} $$
$$ \text{D = 0.5 B} $$
$$ \text{E = 0.5 B} $$

---

But is this really true? Just on the basis of mass, this may be true. But as we have seen before, we need the other numbers to get the best picture. So, let us complete the picture.

---

$$ \text{Question: If we compare the vector forms of A and B to those of C, D, and E, what can we observe?} $$

---

Right away, comparing $A$ to $C$, $D$, and $E$, we can see that:

- $A$ and $C$: Same mass, slightly different volumes. Not exactly the same, but we can safely assume that they are similar.
- $A$ and $D$: $D$ is 2.5 times $A$, as we originally thought! This implies that $D$ is actually $A$, just in more quantity (i.e., mass) and larger (i.e., volume)! **Hint**: Think one bucket made of the same material as the other but two-and-a-half times bigger.
- $A$ and $E$: We thought $E$ was 2.5 times $A$, just like with $D$. However, this is true for the mass, but not for the volume.

If we also do the same for $B$ to $C$, $D$, and $E$, we can see that:

- $B$ and $C$: Different mass, different volumes. $m_C = \frac{m_B}{5}$ but $v_C \not ={\frac{v_B}{5}}$.
- $B$ and $D$: Different mass, different volumes. $m_C = 2.5 m_B$ but $v_C \not ={2.5 v_B}$.
- $B$ and $E$: We thought $E$ was 0.5 times $B$, and we were right! $m_C = 0.5 m_B$ and $v_C = 0.5 v_B$

Based on this, we now know that:

---

#### Assumptions

$$ \textbf{(IV)} $$
$$ \text{C = A âœ…} $$
$$ \text{D = 2.5 A âœ…} $$
$$ \text{E = 2.5 A âŒ} $$
$$ \text{C = 0.2 B âŒ} $$
$$ \text{D = 0.5 B âŒ} $$
$$ \text{E = 0.5 B âœ…} $$

---

### Vector ğŸ”¢ğŸ”¢ğŸ”¢ Comparison âš–ï¸ 2.0

All of this work is exhausting ğŸ˜ª! What if we had a simple way to telling if two or more vectors ğŸ”¢ğŸ”¢ğŸ”¢ were the same (and how similar they are, if they are not exactly the same)?

This is the ***cosine similarity***. With the cosine similarity, we can compare two vectors very easily! The equation is simple!

---

$$ cossim(\vec{A},\vec{B}) = \frac{\sum^N_{i=0}{A_i.B_i}}{|A|.|B|}$$

---

Let us look at an example. Remember:

---

$$ \textbf{(V)} $$
$$ \text{A = [3, 30]} $$
$$ \text{B = [15, 4]} $$
$$ \text{C = [3, 27]} $$
$$ \text{D = [7.5, 75]} $$
$$ \text{E = [7.5, 2]} $$

---

Calculating the cosine similarity for $A$ and $B$:

---

$$ |A| = \sqrt{3^2 + 30^2} = \sqrt{909} = 30.15 $$
$$ |B| = \sqrt{15^2 + 4^2} = \sqrt{241} = 15.52 $$

$$ cossim(\vec{A},\vec{B}) = \frac{(3 \times 15) + (30 \times 4)}{30.15 \times 15.52} = 0.353$$

---

If we calculate the cosine similarity for $A$ and $C$:

---

$$ |A| = \sqrt{3^2 + 30^2} = \sqrt{909} = 30.15 $$
$$ |C| = \sqrt{3^2 + 30^2} = \sqrt{909} = 30.15 $$

$$ cossim(\vec{A},\vec{C}) = \frac{(3 \times 3) + (30 \times 30)}{30.15 \times 30.15} = 1.000$$

---

And how about the cosine similarity for $B$ and $E$?

---

$$ |B| = \sqrt{15^2 + 4^2} = \sqrt{241} = 15.52 $$
$$ |E| = \sqrt{7.5^2 + 2^2} = \sqrt{60.25} = 7.76 $$

$$ cossim(\vec{B},\vec{E}) = \frac{(15 \times 7.5) + (4 \times 2)}{15.52 \times 7.76} = 1.000$$

ğŸ’¡ Notice how the cosine similarity can easily detect that $E$ is just an exactly scaled-down replica of $B$, and so it knows that they are perfectly similar otherwise ğŸ˜!

---

$$ \text{Exercise: Calculate the cosine similarities for all the pairs of vectors ğŸ”¢ğŸ”¢ğŸ”¢. Compare these cosine similarities with the assumptions we made.} $$

---

Now that we have constructed the vocabulary, we know that any word we (or the LLM) encounter can be represented in terms of these unique symbols in the vocab. Now, we need to represent these tokens ğŸ”µğŸ”´ as numbers ğŸ”¢, since the LLM does not know text the way we do.

The solution is simple: assign a unique integer ğŸ”¢ index to each unique token ğŸ”µğŸ”´ in the vocabulary. That way, whenever the LLM sees a specific integer ğŸ”¢ number, we know exactly what token ğŸ”µğŸ”´ it is referring to ğŸ˜! Let us look at a simple example using the low-level vocab:

Say we have the low-level vocab made of letters of the English alphabet:

---

$$\text{Vocab = \{a, b, c, d..., z\}}$$

---

All we need to do is represent each token ğŸ”µğŸ”´ by an integer ğŸ”¢ starting from zero 0ï¸âƒ£. In practically all cases, we number then according to their position e.g. *a* is 0, *b* is 1 and so on.

---

$$\text{Vocab = \{0, 1, 2, 3..., 25\}}$$

---

That is all (mostly ğŸ˜)!

---

## Phase 3ï¸âƒ£: Embeddings ğŸ”¢ğŸ”¢ğŸ”¢ as Vectors ğŸ”¢ğŸ”¢ğŸ”¢

Now we have the ability to represent objects as vectors ğŸ”¢ğŸ”¢ğŸ”¢ ğŸ˜! This idea applies to both concrete (i.e., physical) and abstract (i.e., non-physical) objects. For instance, we have just used vectors ğŸ”¢ğŸ”¢ğŸ”¢ to represent concrete objects like materials. We can also represent abstract ideas ğŸ’¡ğŸ’­ and concepts e.g., velocity, acceleration.

---

$$ \text{Question ğŸ¤”:} $$
$$ \text{If we can apply vectors ğŸ”¢ğŸ”¢ğŸ”¢ to abstract ideas like velocity and acceleration, ...} $$
$$ \text{... can we apply them to words and tokens ğŸ”µğŸ”´ ğŸ¤”?} $$

---

Yes, we can ğŸ˜! In fact, that is the way to go ğŸ˜! We construct a vector ğŸ”¢ğŸ”¢ğŸ”¢ for a token ğŸ”µğŸ”´ such that each dimension of the vector ğŸ”¢ğŸ”¢ğŸ”¢ encodes some kind of information about said token ğŸ”µğŸ”´ ğŸ˜€! This vector ğŸ”¢ğŸ”¢ğŸ”¢ is an attempt to represent the meaning of this token ğŸ”µğŸ”´. So ...

---

$$ \text{Question ğŸ¤”: How do we compare two tokens ğŸ”µğŸ”´ or words to see if they have similar meanings ğŸ¤”?} $$
$$ \text{Answer: Using the cosine similarity ğŸ˜€!} $$

---

The cosine similarity comes to the rescue ğŸ˜€!

We have already seen how we can use the cosine similarities to calculate the similarity between vectors ğŸ”¢ğŸ”¢ğŸ”¢. If we can represent the meaning of tokens ğŸ”µğŸ”´ as vectors ğŸ”¢ğŸ”¢ğŸ”¢, then we can compare these meanings by comparing the vectors ğŸ”¢ğŸ”¢ğŸ”¢ using the **cosine similarity**!

These vectors that represent the meaning of a token ğŸ”µğŸ”´? We have a name for them: **Embeddings**!

---

## Summary

1ï¸âƒ£ Vectors ğŸ”¢ğŸ”¢ğŸ”¢ help us represent the world in more completion.

2ï¸âƒ£ We can compare these vectors ğŸ”¢ğŸ”¢ğŸ”¢ using metrics like cosine similarity.

3ï¸âƒ£ If we can somehow represent the meaning of our tokens ğŸ”µğŸ”´ using vectors ğŸ”¢ğŸ”¢ğŸ”¢ ...

4ï¸âƒ£ ... we can then compare the meaning of these tokens ğŸ”µğŸ”´ in vector ğŸ”¢ğŸ”¢ğŸ”¢ format using this cosine similarity ğŸ˜!

5ï¸âƒ£ These vectors ğŸ”¢ğŸ”¢ğŸ”¢ that represent the meaning of a token are known as $\text{Embeddings}$.

So, we now know how embeddings can be useful. We now know that they can help us encode the meaning of tokens ğŸ”µğŸ”´. But where do they come from? And how do we even get them? In the next lesson, we will dive deeper into the concepts of embeddings ğŸ˜€!

---

## Further Materials

1. [Vector embeddings tutorial (YouTube) ğŸ“½ï¸](https://www.youtube.com/watch?v=yfHHvmaMkcA)
2. [A Beginner's Guide to Vector Embeddings (YouTube) ğŸ“½ï¸](https://www.youtube.com/watch?v=NEreO2zlXDk)
3. [Tokens & Embeddings (Blog) ğŸ“](https://rohitkhattar.substack.com/p/tokens-and-embeddings-the-hidden)
4. [From Tokens to Embeddings (Blog) ğŸ“](https://ml-digest.com/architecture-training-of-the-embedding-layer-of-llms/)

---

## Spotlight of the Day

<br>

<center><img src=26.png alt="Spotlight Image"></img></center>
