
# Day 0ï¸âƒ£2ï¸âƒ£3ï¸âƒ£: Language Models (LMs)

---

$$\text{"If you talk to a man in a language he understands, that goes to his head ğŸ™‚â€â†•. If you talk to him in his language, that goes to his heart ğŸ’“."}$$
$$\text{Nelson Mandela}$$

---

$$\text{â€œThe limits of my language mean the limits of my world ğŸŒ.â€}$$
$$\text{Ludwig Wittgenstein}$$

---

## Synopsis

- [Day 0ï¸âƒ£2ï¸âƒ£3ï¸âƒ£: Language Models (LMs)](#day-0ï¸âƒ£2ï¸âƒ£3ï¸âƒ£-language-models-lms)
  - [Synopsis](#synopsis)
  - [Language for Generative AI ğŸ«§ğŸ¤–](#language-for-generative-ai-)
  - [Language modelling](#language-modelling)
  - [Language Model Classification](#language-model-classification)
    - [1ï¸âƒ£ Gap Location](#1ï¸âƒ£-gap-location)
      - [ğŸ…°ï¸ Causal language model](#ï¸-causal-language-model)
      - [ğŸ…±ï¸ Masked language model](#ï¸-masked-language-model)
    - [2ï¸âƒ£ External Information?](#2ï¸âƒ£-external-information)
      - [1ï¸âƒ£ Base LLMs](#1ï¸âƒ£-base-llms)
      - [2ï¸âƒ£ Augmented LLMs](#2ï¸âƒ£-augmented-llms)
    - [3ï¸âƒ£ Type of External Information](#3ï¸âƒ£-type-of-external-information)
      - [1ï¸âƒ£ Unimodal LLMs](#1ï¸âƒ£-unimodal-llms)
      - [2ï¸âƒ£ Multimodal LLMs](#2ï¸âƒ£-multimodal-llms)
    - [4ï¸âƒ£ Number of Chances](#4ï¸âƒ£-number-of-chances)
      - [1ï¸âƒ£ Zero-Shot](#1ï¸âƒ£-zero-shot)
      - [2ï¸âƒ£ Few-Shot](#2ï¸âƒ£-few-shot)
  - [Summary ğŸ“](#summary-)
  - [Further Materials ğŸ“‘](#further-materials-)
  - [Spotlight of the Day ğŸ”¦](#spotlight-of-the-day-)

---

## Language for Generative AI ğŸ«§ğŸ¤–

Previously, we have covered the use of generative AI ğŸ«§ğŸ¤–. We know that it allows us to be creative. We also know that leveraging it is as simple as telling it what we want. But is it really that simple? And when we actually do tell it what we want, how can it understand us?

For the first question, the short answer is *"Yes"*, and the fuller answer is *"Not really"*. More on this later. As for the second question, the answer is the focus of this and the next few lessons ğŸ¤ª!

---

## Language modelling

In the present day, most generative AI is powered by a technique known as **language modelling**. In basic language (pun intended ğŸ˜œ), language modelling is an attempt to study linguistic structure by leveraging the principles of probability. For those of us who remember Sherlock Holmes of fabled legend, we will recall that he has done something like this, where he deduces that the letter **E** is the most common letter in English.

The result of language modelling is a **language model** (LM) which estimates the probability of a word or sequence of words, given other words. Let us give an example. Fill the gap below:

---

<center>
<strong>
Victor is <em>a</em> __________.
</strong>
</center>

---

Anyone with even a basic command of the English language can probably come up with an accurate guess to fill that gap. But here's the interesting thing: if 100 people tried to fill the gap, we'd end up with multiple different possible answers, and they would likely all be correct ğŸ¤¯! This is a hallmark of creativity: all roads ğŸ›£ï¸ lead to Rome ğŸ›ï¸, there is rarely a wrong answer, and there is often a lot of correct answers!

This is language modelling in action: predicting the next likely word based on other words available! This is essentially *fill-the-gap*, a game that little kids play in school ğŸ¤¯!

Now, let's try another example with a new gap:

---

<center>
<strong>
Victor is <em>an</em> __________.
</strong>
</center>

---

Looking at the new gap to fill, it is clear, once again, that the options that might fit the first gap would definitely not befit the second. On a balance of probability, words that were likely to appear in the first gap would be much less likely to appear in the second. We can attribute this judgment to the presence of the *an* article which is substituted for the original *a*.

Essentially, we are trying to figure out the most likely next word given a set of previous words, including the article *an*! And we can do this because we have an internal language model, which is a generative intelligence (:wink: ğŸ˜œ) that allows us to create sentences!

---

## Language Model Classification

Now we know that language modelling is essentially a fill-the-gap game. We can classify language models and the tasks they solve based on the kinds of gaps we are trying to fill. These classifications include:

1ï¸âƒ£ Gap location
2ï¸âƒ£ Whether or not external information is needed to fill the gap
3ï¸âƒ£ If external information is needed to fill the gap, what type of information is it?
4ï¸âƒ£ How many chances are there to fill the gaps?

---

### 1ï¸âƒ£ Gap Location

Now we know that language modelling is essentially a fill-the-gap game. However, we know that in such games, the gap can appear at any position in the sentence.

For instance:

---

<center>

<strong>
Victor is <em>an</em> __________.
</strong>

<br>

<strong>
_______ is the fairest of them all_
</strong>

<br>

<strong>
_______ is <em>a</em> teacher by _______ and <em>a</em> tailor by ________.
</strong>

<br>

<strong>
_________ is <em>an</em> engineering _________.
</strong>

</center>

---

I will leave the task of completing the gaps to you, but I trust you have picked up the salient point: ***the gaps can appear at the beginning, end or even the middle of the sentence***. Even more, they can appear at multiple of these positions simultaneously.

Each of these fill-the-gap problems can be solved using a language model, and the location of the gap determines what kind of language model we are dealing with. Beyond gap location, we can also classify language models based on the information we require in order to fill the gap i.e., information type and source. More on this later.

For now, based on the gap location, we have:

---

#### ğŸ…°ï¸ Causal language model

Also known as **Autoregressive Language Models**.

This type of language model deals with gaps at the end of the sentence. This is the most common form of language modelling task we can encounter. When people talk of language models without specifying, they are usually talking about this type.

<br>

<center>

<strong>
Victor is <em>an</em> __________.
</strong>

</center>

<br>

These language models are mostly focused on ***decoding*** and completing the gap. **Hint**: More on this later.

---

#### ğŸ…±ï¸ Masked language model

Also known as **Autoencoding LLMs**.

This type of language model deals with gaps anywhere else other than (and possibly including) the end of the sentence. For example:

<br>

<center>

<strong>
_______ is <em>a</em> teacher by _______ and <em>a</em> tailor by ________.
</strong>

</center>

<br>

Language models of this type are focused less on completing the gap, and more on ***encoding*** information about language structure. **Hint**: More on this later as well.

---

### 2ï¸âƒ£ External Information?

In order to fill the gap present in the query, external clues ğŸ§© might be needed. Where do these clues ğŸ§© come from? Some times these clues ğŸ§© are contained in the text already. However, some times, these clues ğŸ§© may be provided from external sources of information.

#### 1ï¸âƒ£ Base LLMs

These LLMs only need their input in order to generate their responses. They can answer questions like:

---

<br>

<center>

<strong>
Sonia is <em>a</em> teacher by profession. Her job is to ________ students.
</strong>

</center>

<br>

---

#### 2ï¸âƒ£ Augmented LLMs

These LLMs require some augmentation with external information in order to successfully process input. Most LLMs you will encounter have been augmented in some way.

A popular example of this is called **In-context learning (ICL)**. In order to answer the question successfully, we can provide the LLM with some examples, which will give it some idea of how to proceed. Examples:

---

<br>

<center>

<strong>
Review: The movie was fantastic â†’ Sentiment: Positive
</strong>
<br>
<strong>
Review: The storyline was wack  â†’ Sentiment: Negative
</strong>
<br>
<strong>
Review: The music was pleasant  â†’ Sentiment: ________
</strong>

</center>

<br>

---

### 3ï¸âƒ£ Type of External Information

If external information is indeed needed to aid the LLM, wht kind of information is it?

#### 1ï¸âƒ£ Unimodal LLMs

These LLMs operate on one type of data ğŸ’½ğŸ’¾: usually text ğŸ”¡. As such, they can only process text.

#### 2ï¸âƒ£ Multimodal LLMs

The LLMs operate on multiple types of data ğŸ’½ğŸ’¾, e.g., text ğŸ”¡ & images ğŸ–¼ï¸, text ğŸ”¡ & videos ğŸ“½ï¸ etcetera. An example of this are VLMs (Vision-Language Models).

---

### 4ï¸âƒ£ Number of Chances

Think of yourself as the teacher ğŸ§‘â€ğŸ« and the LLM as a student ğŸ‘©â€ğŸ“. Imagine you are trying to evaluate the student ğŸ‘©â€ğŸ“ on a question. They may or may not get it right on the first try. Do we give them more chances, and if so how many? This is actually very similar to in-context learning.

#### 1ï¸âƒ£ Zero-Shot

Also known as **Single-shot**.

Involves giving the LLM input without specifying examples.

#### 2ï¸âƒ£ Few-Shot

Also known as **Multi-shot**.

Involves providing the LLM with one or more examples of the desired task before asking it to perform. This can range from one example (one-shot) to several (few-shot).

---

## Summary ğŸ“

We have learnt how to classify LLMs and the tasks they can perform. There are other ways of doing this (we will encounter sone of them later), but we are off to a good start ğŸ˜ğŸ¥³.

Now, we know what language models do. They help us model the structure of a language in a mathematical way using probability. But what does this mean? And how does this happen? We will see this soon.

---

## Further Materials ğŸ“‘

1. [Language with Sherlock Holmes ğŸš¬ğŸ© (Blog) ğŸ“](https://blog.oup.com/2020/01/codes-and-ciphers/)
2. [Large Language Models: Concepts (Blog) ğŸ“](https://vitalflux.com/large-language-models-concepts-examples/)
3. [In-context Learning (ICL) (Blog) ğŸ“](https://www.ibm.com/think/topics/in-context-learning)
4. [Stanford AI Guide to In-context Learning (Blog) ğŸ“](https://ai.stanford.edu/blog/understanding-incontext/)
5. [Single- and Multi-shot LLM Prompting (Blog) ğŸ“](https://artificialintelligenceschool.com/single-shot-vs-multi-shot-a-comprehensive-guide-to-llm-evaluation-techniques/)
6. [Unimodal vs. Multimodal LLMs (Blog) ğŸ“](https://www.dataknobs.com/generativeai/10-llms/multi-modal-llm/4-multimodal-vs-unimodal-llms-key-differences-and-applications.html)
7. [Vision-Language Models (Blog) ğŸ“](https://www.ibm.com/think/topics/vision-language-models)

---

## Spotlight of the Day ğŸ”¦

<br>

<center><img src=23.png alt="Spotlight Image"></img></center>
